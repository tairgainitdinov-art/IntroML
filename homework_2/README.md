# Отчёт по лабораторной работе 2
**Тема:** Реализация алгоритма KNN с нуля и исследование линейной регрессии  

---

## Цель работы

1. Реализовать алгоритм ближайших соседей (**K-Nearest Neighbors, KNN**) с нуля.  
2. Проверить корректность работы алгоритма на тестовых данных.  
3. Проанализировать поведение KNN при различных параметрах (количество соседей, метрика расстояния и т.д.).  
4. Ознакомиться с принципами **линейной регрессии** и провести эксперименты на реальных данных.  

---

## Ход работы

1. **Выбор данных.**  
   Для экспериментов использовался датасет *Palmer Penguins*, содержащий характеристики трёх видов пингвинов: *Adelie*, *Chinstrap* и *Gentoo*.  
   В наборе данных были признаки: длина и глубина клюва, длина ласта, масса тела, пол, изотопные характеристики и др.

2. **Предобработка данных.**  
   - Проверены пропущенные значения — отсутствующие записи были удалены.  
   - Категориальные признаки закодированы численно с помощью *OrdinalEncoder*.  
   - Целевая переменная (`Species`) закодирована как:  
     `Chinstrap = 0`, `Gentoo = 1`, `Adelie = 2`.  

3. **Реализация алгоритма KNN.**  
   - Реализована функция поиска ближайших соседей без использования готовых реализаций scikit-learn.  
   - В качестве метрики использовалось **евклидово расстояние**.  
   - Классификация нового объекта производилась на основе **голосования** ближайших соседей.  
   - Реализована визуализация решающих поверхностей для различных значений параметра `k`.  

4. **Проверка корректности.**  
   - Алгоритм протестирован на искусственных данных (тест-кейсы).  
   - Сравнены результаты собственной реализации с библиотечной функцией `KNeighborsClassifier` из scikit-learn.  
   - При совпадении метрик (accuracy, confusion matrix) сделан вывод о корректности реализации.  

5. **Исследование линейной регрессии.**  
   - Проведены эксперименты с *LinearRegression* из scikit-learn.  
   - Рассмотрены понятия переобучения и регуляризации.  
   - Проведено сравнение предсказаний и ошибок на тренировочной и тестовой выборках.  

---

## Результаты

- Реализованный алгоритм KNN показал результаты, сопоставимые с библиотечной реализацией sklearn.  
- На визуализациях видно, как при малом `k` классификатор сильно "подстраивается" под обучающие данные (эффект переобучения).  
- При увеличении `k` решающая поверхность сглаживается, повышается устойчивость модели.  
- Линейная регрессия корректно аппроксимировала данные и показала зависимость признаков от целевой переменной.  
- Проведён анализ метрик точности и ошибок на тестовой выборке.  

---

## Выводы

1. Реализация KNN "с нуля" позволила глубже понять логику работы алгоритма — от вычисления расстояний до принятия решений по голосованию соседей.  
2. Алгоритм KNN прост в реализации, но чувствителен к выбору параметра `k` и масштабу признаков.  
3. Визуализация решающих поверхностей наглядно показала, как изменяется граница классов при разных `k`.  
4. Линейная регрессия остаётся базовой, но мощной моделью, хорошо подходящей для анализа зависимостей и прогнозирования.  
5. Лабораторная работа помогла закрепить принципы построения и оценки моделей машинного обучения.

---

**Автор:** *[указать своё имя]*  
**Дата выполнения:** *[указать дату]*  
